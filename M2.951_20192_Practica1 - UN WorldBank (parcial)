import builtwith
import whois
from bs4 import BeautifulSoup
import requests
import re
import numpy as np
import pandas as pd
from nested_dict import nested_dict
import time
import csv
import itertools
import sys
import os.path


# Obtenim informació sobre amb quines tecnologies s'ha elaborat el lloc web.
# print(builtwith.builtwith("http://data.un.org/en/index.html"))
# print(builtwith.builtwith("http://data.un.org/en/iso/af.html"))

# Obtenim informació sobre el propietari del lloc web:
# print(whois.whois("http://data.un.org/en/index.html"))

# Funcions d'utilitats
from practica1funcions import *

# Descàrrega del lloc web "index", que conté les URLs d'interés
page_index = llegir_pagina("http://data.un.org/en/index.html","UNData.index.html")

# Guardem el contingut de l'objecte "page" en un objecte "soup":
soup_index = BeautifulSoup(page_index, features="html.parser")

# Observem la estructura imbricada del document html
#print(soup_index.prettify())

# Creem una lista amb les URL de cada país (enllaços en format "iso/<pais>.html")
list_url = []
for link in soup_index.find_all('a',href=re.compile("iso")):
    list_url.append(link['href'])

print(list_url)

# Bucle de descàrrega per cada URL:

"""
list_soup = []
for i in list_url:
    page_country = requests.get(i)
    soup_country = BeautifulSoup(page_country.content, features="html.parser")
    list_soup.append(soup_country)
    time.sleep(4)

print(list_soup[0])
print(type(list_soup[0]))

"""

# Del bucle hem obtingut una llista que conté objectes "soup", un per cada URL descarregada.
# Carreguem els 3 primers països


# list_url = ["http://data.un.org/en/iso/af.html","http://data.un.org/en/iso/al.html","http://data.un.org/en/iso/dz.html"]
list_soup = []

for i, url in enumerate(list_url[:3]):
    filename = os.getcwd() + "/" + url.replace("/", "_")
    fp = llegir_pagina("http://data.un.org/en/" + url, filename)
    soup = BeautifulSoup(fp, features="html.parser")
    list_soup.append(soup)



# print(soup_af.prettify())

# NOTA: linies 20 a 73 es modificaran per tal de guardar arxius en disc i que el programa els reconegui,
# #i no fer un ."get" repetidament


# Creem un diccionari on la clau contindrà el noms del país, i com a valor contindrà un diccionari on es
# donarà l'any i el valor de tots els index (hi ha 44 indicadors diferents):
# {"Pais_1": {"2005": [valor_indicador_1, valor_indicador_2,...,valor_indicador_44],
#             "2010": [valor_indicador_1, valor_indicador_2,...,valor_indicador_44],
#             "2019": [valor_indicador_1, valor_indicador_2,...,valor_indicador_44]},
# {"Pais_2": {"2005": [valor_indicador_1, valor_indicador_2,...,valor_indicador_44],
#             "2010": [valor_indicador_1, valor_indicador_2,...,valor_indicador_44],
#             "2019": [valor_indicador_1, valor_indicador_2,...,valor_indicador_44]},
# .....}


# NOTA: Es fara una o varies `encapsulations` en funcions, que prendran directament la url de la
# llista "list_url"

def de_soup_a_dict(list_soup):
    """Funció que pren la llista d'objectes soup i els converteix a diccionari.
    Es modificarà per que prengui la llista d'url"""
    dict_indicators = nested_dict()
    for soup in list_soup:
        list_years = []
        list_2005 = []
        list_2010 = []
        list_2019 = []
        for k, m in enumerate(soup.find_all('tbody')):
            # S'obté el nom de país i la llista d'anys pels quals hi ha dades
            if k == 0:
                cell_country_name = m.find_all('td')
                for q in range(len(cell_country_name)):
                    if q == 3:
                        country_name = cell_country_name[q].find(text=True)
                        # Imprimim nom com a mesura de control
                        # print(country_name)
                        for i in soup.find_all('thead'):
                            cell_year = i.find_all('td')
                            for j in range(1, len(cell_year)):
                                list_years.append(cell_year[j].find(text=True))
            # Obtenim els valors dels indicadors, que col·loquem en tres llistes segons l'any
            if k >= 2 and (k <= len(soup.find_all('tbody')) - 3):
                cell_indicator = m.find_all('td')
                list_td = list(range(len(cell_indicator)))
                for n in list_td[1:len(cell_indicator):4]:
                    cell_small_1 = cell_indicator[n].find('small')
                    text_small_1 = cell_small_1.find(text=True)
                    if text_small_1 == "...":
                        text_small_1 = "NA"
                        list_2005.append(text_small_1)
                    else:
                        list_2005.append(text_small_1)
                for n in list_td[2:len(cell_indicator):4]:
                    cell_small_2 = cell_indicator[n].find('small')
                    text_small_2 = cell_small_2.find(text=True)
                    if text_small_2 == "...":
                        text_small_2 = "NA"
                        list_2010.append(text_small_2)
                    else:
                        list_2010.append(text_small_2)
                for n in list_td[3:len(cell_indicator):4]:
                    cell_small_3 = cell_indicator[n].find('small')
                    text_small_3 = cell_small_3.find(text=True)
                    if text_small_3 == "...":
                        text_small_3 = "NA"
                        list_2019.append(text_small_3)
                    else:
                        list_2019.append(text_small_3)
        dict_indicators[country_name][list_years[0]] = list_2005
        dict_indicators[country_name][list_years[1]] = list_2010
        dict_indicators[country_name][list_years[2]] = list_2019

    return dict_indicators


# Pendent:
# - Dividir els indicadors amb 2 valors en 2 indicadors diferents
# - Eliminar "~"
# - Passar a valors numèrics els valors dels indicadors
# - No tots els paisos tenen els mateixos indicadors

dict_indicators_UN = de_soup_a_dict(list_soup)
print(dict_indicators_UN)

# print(dict_indicators_UN)

# Creem una llista que contingui el nom dels atributs (header del csv):
columns = ["country", "year"]

for k, m in enumerate(list_soup[0].find_all('tbody')):
    if k >= 2 and (k <= len(list_soup[0].find_all('tbody')) - 3):
        cell_indicator = m.find_all('td')
        for n in range(len(cell_indicator)):
            list_1 = list(range(len(cell_indicator)))
            if n in list_1[:len(cell_indicator):4]:
                text = cell_indicator[n].find(text=True)
                cell_small = cell_indicator[n].find('small')
                text_small = cell_small.find(text=True)
                columns.append(text + text_small)
print(columns)
print(len(columns))

# Convertir els diccionaris a DataFrame per integrar els indicadors del Banc Mundial

lll = list(dict_indicators_UN.items_flat())

l = list()

for i in lll:
    j = i[1][0:34].copy()
    j.insert(0, i[0][0])
    j.insert(1, i[0][1])
    l.append(j)

dfUN = pd.DataFrame(l, columns=columns)

# Incorporar indicadors obtinguts via API del BancMundial
# Pendent: Convertir noms de paisos a codi ISO3 (o a la inversa) per que el merge UN - Banc Mundial funcioni.

indicadors_BM = ["SP.DYN.LE00.IN", "SP.POP.TOTL"]

for indBM in indicadors_BM:
    print(indBM)
    df1 = llegir_indicador_world_bank(indBM)
    dfUN = dfUN.merge(df1[df1['year'].isin(['2005', '2010', '2019'])], how='outer', on=['country', 'year'])

# PENDENT: Pas final guardem el DataFrame dfUN com a .csv
print(dfUN.head())
